#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{listings}
\lstloadlanguages{Python}
\lstset{language=Python} 
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Free Lunch Project
\end_layout

\begin_layout Title

\size small
Model selection and parameter tuning as an optimization problem
\end_layout

\begin_layout Author
Manuel Del Verme
\begin_inset Newline newline
\end_inset

1769408
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
what is this work inspired by
\end_layout

\begin_layout Standard
The work was inspired by the AutoML Challenge.
 The challenge participants worked to develop fully automatic “black-box”
 learning machines for feature-based classification and regression problems.
 In each new round, the participants’ code underwent a blind test on 5 new
 datasets.
 This was regardless of the type of dataset, which included a wide range
 in level of complexity.
\end_layout

\begin_layout Standard
During the competition the participants were able to manually tune their
 models for over a month during 
\begin_inset Quotes eld
\end_inset

Tweakathon
\begin_inset Quotes erd
\end_inset

 phases, when we look closely at the results of the challenge, we can see
 that there is still significant room for improvement for the algorithms.
 For one thing, there’s a significant gap between Tweakathon and AutoML
 results, indicating that the “automatic” algorithms can be further optimized.
 Nonetheless, this challenge has resulted in a leap forward for the field
 in terms of automation.
\end_layout

\begin_layout Section
past work
\end_layout

\begin_layout Subsection
The automated statistician
\end_layout

\begin_layout Standard
Automating the process of statistical modeling would have a tremendous impact
 on fields that currently rely on expert statisticians, machine learning
 researchers, and data scientists.
 Such expertise increasingly in demand, especially with the growth in data
 gathering in the sciences and in industry.
\end_layout

\begin_layout Standard
The Automatic Statistician is a system which explores an open-ended space
 of possible statistical models to discover a good explanation of the data.
 The system is based on reasoning over an open-ended language of nonparametric
 models using Bayesian inference.
\end_layout

\begin_layout Section
experiment
\end_layout

\begin_layout Subsection
Goal
\end_layout

\begin_layout Standard
the goal was to automatically generate a model to explain the cross validated
 variance, aiming to minimize the cross-validated root-mean-squared-errors
 (RMSE) over a biological dataset mappign M1-neuron spike frequency to hand
 velocities 
\begin_inset Formula $(\dot{x},\dot{y})$
\end_inset

 of a *RACEHERE* monkey.
\end_layout

\begin_layout Section
set up
\end_layout

\begin_layout Subsection
configuration space
\end_layout

\begin_layout Standard
A configuration space of:
\end_layout

\begin_layout Itemize
Adaboost
\end_layout

\begin_layout Itemize
decision tree
\end_layout

\begin_layout Itemize
extra trees
\end_layout

\begin_layout Itemize
gradient boosting
\end_layout

\begin_layout Itemize
liblinear svr
\end_layout

\begin_layout Itemize
sgd
\end_layout

\begin_layout Itemize
random forest
\end_layout

\begin_layout Standard
and their parameter spaces.
\end_layout

\begin_layout Standard
Some models were excluded a priori: - k_nearest_neighbors, ridge_regression,
 ard_regression (based on prior experiments as they didn't give good results
 in prior works [Ari][Josh]), other models (their quadratic time complexity
 would have increased the training time by a wide margin) were excluded
 for more practical reasons): gaussian process, non linear-SVR.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
the configuration space for non-ensemble models is unlimited, there are
 7 possible algorithms with and their continuous parameter spaces (approx.
 two parameters per model were searched), considering that a single algorithm
 can take a long time to converge and considering that the naive solution
 would be to wait for all of them to converge and further considering that
 ensemble of models have proven to achieve better explainations, it's clear
 that we need a very data efficient (where data is the underlying selected
 model) optimisation algorithm (adding time efficiency constraints on the
 searched models degrades the final performance).
\end_layout

\begin_layout Standard
Alas data efficient algorithms are usually computationally expensive, recently
 this problem was tackled by bayesian optimisation using Gaussian processes
 (GPs), other approaches use random forest, approaches framed as bandits
 etc.
\end_layout

\begin_layout Standard
The project is based on the Freeze-Thaw Bayesian Optimization approach.
\end_layout

\begin_layout Section
Freeze-thaw Bayesian optimisation (FTBO)
\end_layout

\begin_layout Standard
In machine learning, the term “training” is used to describe the procedure
 of fitting a model to data.
 In many popular models, this fitting procedure is framed as an optimization
 problem, in which a loss is minimized as a function of the parameters.
 In all but the simplest machine learning models, this minimization must
 be performed with an iterative algorithm such as stochastic gradient descent
 or the nonlinear conjugate gradient method.
 Another aspect of training involves fitting model “hyperparameters.” These
 are parameters that in some way govern the model space or fitting procedure;
 in both cases they are typically difficult to minimize directly in terms
 of the training loss and are usually evaluated in terms of generalization
 performance via held-out data.
 Hyperparameters are often regularization penalties such as `p norms on
 model parameters, but can also capture model capacity as in the number
 of hidden units in a neural network.
 These hyperparameters help determine the appropriate bias-variance tradeoff
 for a given model family and data set.
 On the other hand, hyperparameters of the fitting procedure govern algorithmic
 aspects of training, such as the learning rate schedule of stochastic gradient
 descent, or the width of a Monte Carlo proposal distribution.
 The goal of fitting both kinds of hyperparameters is to identify a model
 and an optimization procedure in which successful minimization of training
 loss is likely to result in good generalization performance.
 When a heldout validation set is used to evaluate the quality of hyperparameter
s, the overall optimization proceeds as a double loop, where the outer loop
 sets the hyperparameters and the inner loop applies an iterative training
 procedure to fit the model to data.
 Often this outer hyperparameter optimization is performed by hand, which—even
 if rigorously systematized— can be a difficult and laborious process.
 Simple alternatives include the application of heuristics and intuition,
 grid search, which scales poorly with dimension, or random search [1],
 which is computationally expensive due to the need to train many models.
 In light of this, Bayesian optimization [2] has recently been proposed
 as an effective method for systematically and intelligently setting the
 hyperparameters of machine learning models [3, 4].
 Using a principled characterization of model uncertainty, Bayesian optimization
 attempts to find the best hyperparameter settings with as few model evaluations
 as possible.
 One issue with previously proposed approaches to Bayesian optimization
 for machine learning is that a model must be fully trained before the quality
 of its hyperparameters can be assessed.
 Human experts, however, appear to be able to rapidly assess whether or
 not a model is likely to eventually be useful, even when the inner-loop
 training is only partially completed.
 When such an assessment can be made accurately, it is possible to explore
 the hyperparameter space more effectively by aborting model fits that are
 likely to be low quality.
 The goal of this paper is to take advantage of the partial information
 provided by iterative training procedures, within the Bayesian optimization
 framework for hyperparameter search.
 We propose a new technique that makes it possible to estimate when to pause
 the training of one model in favor of starting a new one with different
 hyperparameters, or resuming a partially-completed training procedure from
 an old model.
 We refer to our approach as freeze-thaw Bayesian optimization, as the algorithm
 maintains a set of “frozen” (partially completed but not being actively
 trained) models and uses an information-theoretic criterion to determine
 which ones to “thaw” and continue training.
\end_layout

\begin_layout Standard
Our approach hinges on the assumption that, for many models, the training
 loss during the fitting procedure roughly follows an exponential decay
 towards an unknown final value.
 We build a Bayesian nonparametric prior around this assumption by developing
 a new kernel that is an infinite mixture of exponentiallydecaying basis
 functions, with the goal of characterizing these training curves.
 Using this kernel with a novel and efficient temporal Gaussian process
 prior, we are able to forecast the final result of partially trained models
 and use this during Bayesian optimization to determine the most promising
 action.
 We demonstrate that freeze-thaw Bayesian optimization can find good hyperparame
ter settings for many different models in considerably less time than ordinary
 Bayesian optimization
\end_layout

\begin_layout Itemize
Components of the algorithm
\end_layout

\begin_layout Itemize
Anytime interruptible base learning algorithms
\end_layout

\begin_layout Itemize
Evaluation of base learners
\end_layout

\begin_layout Itemize
A learning curve model – infinite mixture of 
\series bold
exponential 
\series default
decays GP
\end_layout

\begin_layout Itemize
A model of learning curve asymptotes – standard smooth GP
\end_layout

\begin_layout Itemize
A method for deciding which algorithm to explore further – entropy search
\end_layout

\begin_layout Section
Freeze-thaw ensemble construction (FTEC)
\end_layout

\begin_layout Itemize
Components of the algorithm
\end_layout

\begin_layout Itemize
Base learning algorithms – Most of scikit-learn
\end_layout

\begin_layout Itemize
Evaluation of base learners – Cross validation
\end_layout

\begin_layout Itemize
A learning curve extrapolator – Mixture of exponential decays GP
\end_layout

\begin_layout Subsection
An ensembling method
\end_layout

\begin_layout Itemize
None
\end_layout

\begin_layout Itemize
Sequential
\end_layout

\begin_layout Itemize
Stacking
\end_layout

\begin_deeper
\begin_layout Itemize
how to evaluate models
\end_layout

\begin_layout Itemize
curves
\end_layout

\end_deeper
\begin_layout Itemize
A model (or learning algorithm) mapping individual algorithm performance
 to ensemble performance – A time pressured hack Decision trees
\end_layout

\begin_layout Section
Overfitting problem
\end_layout

\begin_layout Standard
use xval
\end_layout

\begin_layout Section
RL for sequential decision making
\end_layout

\begin_layout Section
correlation between algorithms
\end_layout

\begin_layout Section
..
\end_layout

\begin_layout Section
results
\end_layout

\begin_layout Standard
the result was an ensemble of:
\end_layout

\begin_layout Enumerate
gradient boosting (44%)
\end_layout

\begin_layout Enumerate
linear SVR (28%)
\end_layout

\begin_layout Enumerate
adaboost (12%)
\end_layout

\begin_layout Enumerate
decision_tree (4%)
\end_layout

\begin_layout Standard
The resulting metric for the best resulting model was: 0.7706 (R2) which
 is better than the 0.73127717 of the single classifiers in the same class
 of models.
\end_layout

\begin_layout Subsection
Future works
\end_layout

\begin_layout Standard
Approimated gaussian process, (precomputed)kernel SVR would drastically
 increase the accuraccy while allowing for an acceptable computation time,
 adding a k nearest neighbor classifier to the configuration space could
 be beneficial since it is very different from all other methods and thus
 deserves a small nonzero weight.
\end_layout

\begin_layout Section
references
\end_layout

\begin_layout Itemize
Swersky, K., Snoek, J.
 & Adams, R.
 P.
 Freeze-Thaw Bayesian Optimization.
 arXiv preprint 1406.3896 (2014).
 at http://arxiv.org/abs/1406.3896
\end_layout

\begin_layout Itemize
Hennig, P.
 & Schuler, C.
 J.
 Entropy Search for Information-efficient Global Optimization.
 J.
 Mach.
 Learn.
 Res.
 13, 1809–1837 (2012).
\end_layout

\begin_layout Itemize
Wolpert, D., Stacked Generalization., Neural Networks, 5(2), pp.
 241-259., 1992
\end_layout

\end_body
\end_document
